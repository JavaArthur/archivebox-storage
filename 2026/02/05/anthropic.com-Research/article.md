# Research

Our research teams investigate the safety, inner workings, and societal impacts of AI models – so that artificial intelligence has a positive impact as it becomes increasingly capable.

Research teams:AlignmentEconomic ResearchInterpretabilitySocietal Impacts### Interpretability

The mission of the Interpretability team is to discover and understand how large language models work internally, as a foundation for AI safety and positive outcomes.

### Alignment

The Alignment team works to understand the risks of AI models and develop ways to ensure that future ones remain helpful, honest, and harmless.

### Societal Impacts

Working closely with the Anthropic Policy and Safeguards teams, Societal Impacts is a technical research team that explores how AI is used in the real world.

### Frontier Red Team

The Frontier Red Team analyzes the implications of frontier AI models for cybersecurity, biosecurity, and autonomous systems.

## Project Vend: Phase two

PolicyDec 18, 2025In June, we revealed that we’d set up a small shop in our San Francisco office lunchroom, run by an AI shopkeeper. It was part of Project Vend, a free-form experiment exploring how well AIs could do on complex, real-world tasks. How has Claude&#x27;s business been since we last wrote? 

InterpretabilityOct 29, 2025#### Signs of introspection in large language models

Can Claude access and report on its own internal states? This research finds evidence for a limited but functional ability to introspect—a step toward understanding what&#x27;s actually happening inside these models.

InterpretabilityMar 27, 2025#### Tracing the thoughts of a large language model

Circuit tracing lets us watch Claude think, uncovering a shared conceptual space where reasoning happens before being translated into language—suggesting the model can learn something in one language and apply it in another.

AlignmentFeb 3, 2025#### Constitutional Classifiers: Defending against universal jailbreaks

These classifiers filter the overwhelming majority of jailbreaks while maintaining practical deployment. A prototype withstood over 3,000 hours of red teaming with no universal jailbreak discovered.

AlignmentDec 18, 2024#### Alignment faking in large language models

This paper provides the first empirical example of a model engaging in alignment faking without being trained to do so—selectively complying with training objectives while strategically preserving existing preferences.

## Publications

SearchDateCategoryTitleJan 29, 2026AlignmentHow AI assistance impacts the formation of coding skillsJan 28, 2026AlignmentDisempowerment patterns in real-world AI usageJan 22, 2026AnnouncementsClaude&#x27;s new constitutionJan 19, 2026InterpretabilityThe assistant axis: situating and stabilizing the character of large language modelsJan 15, 2026Economic ResearchAnthropic Economic Index: New building blocks for understanding AI useJan 15, 2026Economic ResearchAnthropic Economic Index report: economic primitivesJan 9, 2026AlignmentNext-generation Constitutional Classifiers: More efficient protection against universal jailbreaksDec 19, 2025AlignmentIntroducing Bloom: an open source tool for automated behavioral evaluationsDec 18, 2025PolicyProject Vend: Phase twoDec 4, 2025Societal ImpactsIntroducing Anthropic Interviewer: What 1,250 professionals told us about working with AI See moreJoin the Research team

See open roles